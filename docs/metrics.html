<!DOCTYPE html>
<html>

<head>
  <meta charset="UTF-8">
  <link rel="stylesheet" href="style.css">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>
    MISS_HIT - MATLAB independent, small & safe, high integrity tools
  </title>
</head>

<body>
  <header>MISS_HIT 0.9.44-dev Code Metrics</header>

  <main>
    <div class="section">
      <a href="index.html">MISS_HIT</a> includes a simple code metric
      tool (<span class="file">mh_metric</span>). It computes code
      metrics and complains if the metrics exceed acceptable levels.
    </div>

    <div class="title">
      <img src="assets/settings.svg" alt="Configuration" />
      <h1>Configuration</h1>
    </div>

    <section>
      <h2>Using MISS_HIT Metric</h2>
      <div>
        This tool works exactly the same as the style checker, and
        re-uses the same <a href="style_checker.html">configuration
        files and mechanism</a>.
      </div>

      <div>
        To compute metrics on a set of files:
        <pre>$ mh_metric my_file.m my_model.slx</pre>
      </div>

      <div>
        MH Metric fully supports MATLAB code embedded inside Simulink
        models. (Unlike MH Style, there is no extra flag needed.)
      </div>

      <div>
        To compute metrics for all files in a directory tree:
        <pre>$ mh_metric src/</pre>
      </div>

      <div>
        To produce metrics in the current directory tree:
        <pre>$ mh_metric</pre>
      </div>

      <div>
        Metrics are reported on standard out. You can also produce a
        JSON report, HTML report or write the metrics to a text-file:
<pre>
$ mh_metrics src --text=metrics.txt
$ mh_metrics src --html=metrics.html
$ mh_metrics src --json=metrics.json
</pre>
        In a future release more formats (e.g. csv, etc.) will be
        supported.
      </div>

      <div>
        Inside a CI environment, this produces too much
        output. Instead you can use the ci option for this:
        <pre>$ mh_metrics src --ci</pre> This mode will not produce an
        overall report, instead only report violations.
      </div>

    </section>

    <section>
      <h2>Enabling and disabling metrics</h2>
      <div>
        To turn off limit checking for a specific metric (the default
        for all metrics), you can use the report directive in the
        config files:
        <pre>metric "npath": report</pre>
      </div>

      <div>
        To completely disable a metric (it will not be measured, nor
        turn up in the final report), you can use the disable
        directive:
        <pre>metric "npath": disable</pre>
      </div>

      <div>
        To completely disable or enable all metrics (useful for large
        projects using complicated hierarchical configurations), you
        can use the wildcard directive with report and disable:
        <pre>metric *: disable</pre>
        <pre>metric *: report</pre>
      </div>
    </section>

    <section>
      <h2>Configuring and enforcing limits</h2>
      <div>
        As indicated above, by default metrics are just
        reported. However you canb also enforce limits on any metric.
        For example, to limit the number of paths for each function,
        you add this this to your configuration file:
        <pre>metric "npath": limit 5</pre>
      </div>

      <div>
        As with other configuration, these limits propagate to
        sub-directories. You can change these limits on a more local
        bases (just like with style rules).
      </div>

      <div>
        In the description for each metric below, the name of the
        metric to use in config files is included in parenthesis.
      </div>
    </section>

    <div class="title">
      <img src="assets/check-square.svg" alt="Justifications" />
      <h1>Justifications</h1>
    </div>

    <section>
      <h2>Pragmas</h2>
      <div>
        Metrics can be justified by placing a justification pragma at
        the level of scope where the violation occurs. Please refer
        to <a href="pragmas.html">MISS_HIT Pragmas</a> for a full
        description of all pragmas understood by MISS_HIT.
      </div>

      <div>
        For example:
        <pre>%| pragma Justify (metric, "npath", "can't be refactored");</pre>
      </div>

      <div>
        Longer justifications can be broken up into several lines:
<pre>
%| pragma Justify (metric, "npath",
%|                 "this cannot be refactored " +
%|                 "and I am going to tell you " +
%|                 "why not...");
</pre>
      </div>

      <div>
        Justifications that are useless generate a warning.
      </div>

      <div></div>
      <h2>Integration with issue-tracking systems</h2>

      <div>
        A special configuration directive <tt>regex_tickets</tt> can
        be placed in the MISS_HIT configuration file. When set, this
        allows MISS_HIT to extract ticket identifiers from
        justifications, which in turn appear in a special section in
        the report.
      </div>

      <div>
	For example, this is how you can integrate with JIRA:
	<pre>regex_tickets: "\b[A-Z]{3,}-[0-9]+\b"</pre>

	Or GitHub issues:
	<pre>regex_tickets: "\B#[0-9]+\b"</pre>

	(Other regular expressions are left as an exercise to the reader.)
      </div>

      <div>
	You can mention tickets freely in justification text, for
	example:
<pre>
%| pragma Justify (metric, "cyc",
%|                 "to be fixed in POTATO-666 or KITTEN-42");
</pre>
      </div>

      <div>
	These then appear in the text or HTML report. This is helpful
	if you want to produce a report that mentions when things are
	going to be fixed.
      </div>

      <div>
        The command-line
        option <tt>--ignore-justifications-with-tickets</tt> can be
        used to ignore any justifications that mention a ticket.
      </div>
    </section>

    <div class="title">
      <img src="assets/bar-chart-2.svg" alt="Metrics" />
      <h1>Metrics</h1>
    </div>

    <section>
      <h2>File metrics</h2>
      <div>
        These metrics are computed for each file.
      </div>

      <h3>Lines ("file_length")</h3>
      <div>
        The number of lines. Should be the equivalent to running the
        standard UNIX tool <span class="file">wc -l</span>. This means
        comments and blank lines are counted.
      </div>

      <div></div>
      <h2>Function metrics</h2>
      <div>
        These metrics are computed for each function, nested function,
        and method.
      </div>

      <h3>McCabe Cyclomatic Complexity ("cyc")</h3>
      <div>
        This measures
        the <a href="http://www.literateprogramming.com/mccabe.pdf">McCabe
        cyclomatic complexity</a>. We have aimed for mlint
        compatibility, instead of doing it "right". Specifically this
        means:
        <ul>
          <li>
            Empty branches "count", even though they should not
            contribute, based on the original definition of the
            metric.
          </li>
          <li>
            Exceptions are generally ignored. Specifically a try-catch
            block is treated like on flat block + one branch. This is
            not correct since every statement in a try block may
            create a jump.
          </li>
        </ul>
      </div>

      <h3>Lines in function ("function_length")</h3>
      <div>
        The number of lines for each function. Note that adding
        together all lines of all functions will likely not add up to
        file_length due to blank lines and comments between functions.
      </div>

      <h3>Path count ("npath")</h3>
      <div>
        This approximates the number of paths through a function. This
        metric is based
        on <a href="https://dl.acm.org/doi/abs/10.1145/42372.42379">NPATH</a>
        and should be similar what other popular metric tools compute.
      </div>

      <div>
        Note that this number can grow very large, very quickly,
        especially if you have a lot of sequential if blocks. Further
        note that this metric is neither an under-approximate or
        over-approximate, but a reasonable compromise.
      </div>

      <div>
        Since the MATLAB language supports raising and catching
        exceptions (including exceptions further down the call tree) a
        safe over-approximate cannot be reasonably computed since
        every single line may raise multiple exceptions.
      </div>

      <h3>Maximum nesting of control structures ("cnest")</h3>
      <div>
        This measures the maximum level of nesting of
        control. Statements that are considered control statements
        are:
        <ul>
          <li>If statement</li>
          <li>Switch statement</li>
          <li>For loops</li>
          <li>While loops</li>
          <li>Exception handlers</li>
        </ul>
      </div>

      <h3>Number of function parameters ("parameters")</h3>
      <div>
        This counts the number of parameters for each function. Both
        inputs and outputs are considered.
      </div>

      <h3>Number of direct globals ("globals")</h3>
      <div>
        This counts the number direct non-transitive globals for each
        function. In other words, it counts how many distinct things
        are mentioned by all global statements in a function.
      </div>

      <div>
        For example this function has exactly one direct global:
<pre>
function result = f1()
    global x
    result = x;
end
</pre>
      </div>

      <div>
        This function also has exactly one <i>direct</i> global. There
        is another global dependency via the call to f1, but it is
        hidden. The metric to measure all globals (direct and hidden)
        will be implemented once we have basic flow analysis working.
<pre>
function result = f2()
    global y
    result = f1() + y;
end
</pre>
      </div>

      <h3>Number of persistent variables ("persistent")</h3>
      <div>
        This counts the number of persistent variables for each
        function. In other words, it counts how many distinct things
        are mentioned by all persistent statements in a function.
      </div>

      <div>
        Persistent variables make testing extremely difficult in
        MATLAB, more so than globals; so it is a really good idea to
        not have too many of them.
      </div>
    </section>

    <div class="title">
      <img src="assets/file-text.svg" alt="JSON Schema" />
      <h1>JSON Report Schema</h1>
    </div>

    <section>
      <h2>Purpose</h2>
      <div>
	The purpose of the JSON report (unlike the text or HTML
	report) is to be easily machine-parseable. It is not intended
	for human consumption. The report schema is guaranteed to be
	stable between minor releases: i.e. function processing a
	report from <tt>a.b.*</tt> should also be able to process a
	report from <tt>a.c.*</tt> as long as c > b.
      </div>
    </section>

    <section>
      <h2>Schema</h2>
      <h3>Top-level</h3>
      <div>
	The entire report is a single JSON object containing two
	members:
	<ul>
	  <li>metrics</li>
	  <li>worst_case</li>
	</ul>
      </div>

      <h3>metrics</h3>
      <div>
	The metric member is a JSON object one member per file. The
	value of each member is another JSON object with the following
	members:
	<ul>
	  <li>file_metrics</li>
	  <li>function_metrics</li>
	</ul>
      </div>

      <h3>file_metrics</h3>
      <div>
	A file_metrics member is a JSON object containing members for
	each file metric (e.g. "file_length"). Each value is a
	metrics_result object described below.
      </div>

      <h3>function_metrics</h3>
      <div>
	A function_metrics member is a JSON object containing members
	for each function, which in turn are JSON objects containing
	members for each function metric (e.g. "cyc"). Each value is a
	metrics_result object described below.
      </div>

      <h3>metrics_result</h3>
      <div>
	A metrics_result is a JSON object containing the following
	members:
	<ul>
	  <li>
	    status - will be one of the following strings:
	    <ul>
	      <li>
		"measured only" - this metric was measured, but no limit
		was enforced
	      </li>
	      <li>
		"checked: ok" - this metric was measured, and is equal
		to or below the configured limit
	      </li>
	      <li>
		"checked: justified" - this metric was measured, and
		was above the configured limit; but a justification
		was supplied
	      </li>
	      <li>
		"checked: fail" - this metric was measured, and was
		above the configured limit
	      </li>
	    </ul>
	  </li>
	  <li>
	    measure - an integer indicating the measured value of the
	    metric
	  </li>
	  <li>
	    limit - (only for checked metrics) - an integer indicating
	    the configured limit
	  </li>
	  <li>
	    justification - (only for checked: justified metrics) - a
	    string containing the justification supplied
	  </li>
	</ul>
      </div>

      <h3>worst_case</h3>
      <div>
	The worst case table is JSON object where each member
	described a metric (function or file). The value of each is a
	list with augmented metric_result objects.
      </div>

      <h3>augmented metric_result</h3>
      <div>
	As metric_result (see above), but with on or two additional
	members:
	<ul>
	  <li>
	    file - a string indicating the file where the metric was
	    measured
	  </li>
	  <li>
	    function - (only for function metrics) - a string
	    indicating the function where the metric was measured
	  </li>
	</ul>
      </div>
    </section>

  </main>

  <footer>
    Content &copy; 2020 Florian Schanda<br>
    Style &copy; 2019-2020 Alina Boboc<br>
    MISS_HIT Metric is licensed under the GPLv3
  </footer>
</body>

</html>
